{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ece5e75",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [21]</a>'.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e55415a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T00:15:24.701111Z",
     "iopub.status.busy": "2021-06-08T00:15:24.700288Z",
     "iopub.status.idle": "2021-06-08T00:15:24.702873Z",
     "shell.execute_reply": "2021-06-08T00:15:24.702290Z"
    },
    "papermill": {
     "duration": 0.034919,
     "end_time": "2021-06-08T00:15:24.703014",
     "exception": false,
     "start_time": "2021-06-08T00:15:24.668095",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "kms_key = \"arn:aws:kms:us-west-2:521695447989:key/6e9984db-50cf-4c7e-926c-877ec47a8b25\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351a49cb",
   "metadata": {
    "papermill": {
     "duration": 0.023936,
     "end_time": "2021-06-08T00:15:24.751267",
     "exception": false,
     "start_time": "2021-06-08T00:15:24.727331",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Amazon SageMaker Object Detection Incremental Training\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Setup](#Setup)\n",
    "3. [Data Preparation](#Data-Preparation)\n",
    "  1. [Download data](#Download-data)\n",
    "  2. [Convert data into RecordIO](#Convert-data-into-RecordIO)\n",
    "  3. [Upload data to S3](#Upload-data-to-S3)\n",
    "4. [Intial Training](#Initial-Training)\n",
    "5. [Incremental Training](#Incremental-Training)\n",
    "6. [Hosting](#Hosting)\n",
    "7. [Inference](#Inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f80494f",
   "metadata": {
    "papermill": {
     "duration": 0.023887,
     "end_time": "2021-06-08T00:15:24.799223",
     "exception": false,
     "start_time": "2021-06-08T00:15:24.775336",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this example, we will show you how to train an object detector by re-using a model you previously trained in the SageMaker. With this model re-using ability, you can save the training time when you update the model with new data or improving the model quality with the same data. In the first half of this notebook ([Intial Training](#Initial-Training)), we will follow the [training with RecordIO format example](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/object_detection_pascalvoc_coco/object_detection_recordio_format.ipynb) to train a object detection model on the [Pascal VOC dataset](http://host.robots.ox.ac.uk/pascal/VOC/). In the second half, we will show you how you can re-use the trained model and improve its quality without repeating the entire training process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f482871d",
   "metadata": {
    "papermill": {
     "duration": 0.023666,
     "end_time": "2021-06-08T00:15:24.846835",
     "exception": false,
     "start_time": "2021-06-08T00:15:24.823169",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Setup\n",
    "\n",
    "To train the Object Detection algorithm on Amazon SageMaker, we need to setup and authenticate the use of AWS services. To begin with we need an AWS account role with SageMaker access. This role is used to give SageMaker access to your data in S3 will automatically be obtained from the role used to start the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5527cdbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T00:15:24.899984Z",
     "iopub.status.busy": "2021-06-08T00:15:24.899450Z",
     "iopub.status.idle": "2021-06-08T00:15:26.376362Z",
     "shell.execute_reply": "2021-06-08T00:15:26.376824Z"
    },
    "papermill": {
     "duration": 1.505319,
     "end_time": "2021-06-08T00:15:26.376979",
     "exception": false,
     "start_time": "2021-06-08T00:15:24.871660",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::521695447989:role/ProdBuildSystemStack-ReleaseBuildRoleFB326D49-QK8LUA2UI1IC\n",
      "CPU times: user 1.13 s, sys: 326 ms, total: 1.46 s\n",
      "Wall time: 1.47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "print(role)\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66010b5f",
   "metadata": {
    "papermill": {
     "duration": 0.024333,
     "end_time": "2021-06-08T00:15:26.464544",
     "exception": false,
     "start_time": "2021-06-08T00:15:26.440211",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We also need the S3 bucket that you want to use for training and to store the tranied model artifacts. In this notebook, we require a custom bucket that exists so as to keep the naming clean. You can end up using a default bucket that SageMaker comes with as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1ff14dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T00:15:26.517939Z",
     "iopub.status.busy": "2021-06-08T00:15:26.517293Z",
     "iopub.status.idle": "2021-06-08T00:15:26.716760Z",
     "shell.execute_reply": "2021-06-08T00:15:26.716223Z"
    },
    "papermill": {
     "duration": 0.227728,
     "end_time": "2021-06-08T00:15:26.716888",
     "exception": false,
     "start_time": "2021-06-08T00:15:26.489160",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "bucket = sess.default_bucket()  # Use the default bucket. You can also customize bucket name.\n",
    "prefix = \"DEMO-ObjectDetection\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54adad3",
   "metadata": {
    "papermill": {
     "duration": 0.024466,
     "end_time": "2021-06-08T00:15:26.766166",
     "exception": false,
     "start_time": "2021-06-08T00:15:26.741700",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Lastly, we need the Amazon SageMaker Object Detection docker image, which is static and need not be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "136b4f09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T00:15:26.819918Z",
     "iopub.status.busy": "2021-06-08T00:15:26.819123Z",
     "iopub.status.idle": "2021-06-08T00:15:26.830183Z",
     "shell.execute_reply": "2021-06-08T00:15:26.830636Z"
    },
    "papermill": {
     "duration": 0.040099,
     "end_time": "2021-06-08T00:15:26.830813",
     "exception": false,
     "start_time": "2021-06-08T00:15:26.790714",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The method get_image_uri has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Defaulting to the only supported framework/algorithm version: 1. Ignoring framework/algorithm version: latest.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "433757028032.dkr.ecr.us-west-2.amazonaws.com/object-detection:1\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "training_image = get_image_uri(sess.boto_region_name, \"object-detection\", repo_version=\"latest\")\n",
    "print(training_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5556090",
   "metadata": {
    "papermill": {
     "duration": 0.025751,
     "end_time": "2021-06-08T00:15:26.882017",
     "exception": false,
     "start_time": "2021-06-08T00:15:26.856266",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Preparation\n",
    "[Pascal VOC](http://host.robots.ox.ac.uk/pascal/VOC/) was a popular computer vision challenge and they released annual challenge datasets for object detection from 2005 to 2012. In this notebook, we will use the data sets from 2007 and 2012, named as VOC07 and VOC12 respectively. Cumulatively, we have more than 20,000 images containing about 50,000 annotated objects. These annotated objects are grouped into 20 categories.\n",
    "\n",
    "While using the Pascal VOC dateset, please be aware of the database usage rights:\n",
    "\"The VOC data includes images obtained from the \"flickr\" website. Use of these images must respect the corresponding terms of use: \n",
    "* \"flickr\" terms of use (https://www.flickr.com/help/terms)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9104baf2",
   "metadata": {
    "papermill": {
     "duration": 0.026028,
     "end_time": "2021-06-08T00:15:26.933378",
     "exception": false,
     "start_time": "2021-06-08T00:15:26.907350",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Download data\n",
    "Let us download the Pascal VOC datasets from 2007 and 2012."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3992f050",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T00:15:26.993271Z",
     "iopub.status.busy": "2021-06-08T00:15:26.992718Z",
     "iopub.status.idle": "2021-06-08T00:15:37.444145Z",
     "shell.execute_reply": "2021-06-08T00:15:37.444614Z"
    },
    "papermill": {
     "duration": 10.485986,
     "end_time": "2021-06-08T00:15:37.444768",
     "exception": false,
     "start_time": "2021-06-08T00:15:26.958782",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-06-08 00:15:27--  http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\r\n",
      "Resolving host.robots.ox.ac.uk (host.robots.ox.ac.uk)... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129.67.94.152\r\n",
      "Connecting to host.robots.ox.ac.uk (host.robots.ox.ac.uk)|129.67.94.152|:80... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed: No route to host.\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-06-08 00:15:30--  http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar\r\n",
      "Resolving host.robots.ox.ac.uk (host.robots.ox.ac.uk)... 129.67.94.152\r\n",
      "Connecting to host.robots.ox.ac.uk (host.robots.ox.ac.uk)|129.67.94.152|:80... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed: No route to host.\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-06-08 00:15:33--  http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar\r\n",
      "Resolving host.robots.ox.ac.uk (host.robots.ox.ac.uk)... 129.67.94.152\r\n",
      "Connecting to host.robots.ox.ac.uk (host.robots.ox.ac.uk)|129.67.94.152|:80... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed: No route to host.\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar: /tmp/VOCtrainval_11-May-2012.tar: Cannot open: No such file or directory\r\n",
      "tar: Error is not recoverable: exiting now\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar: /tmp/VOCtrainval_06-Nov-2007.tar: Cannot open: No such file or directory\r\n",
      "tar: Error is not recoverable: exiting now\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar: /tmp/VOCtest_06-Nov-2007.tar: Cannot open: No such file or directory\r\n",
      "tar: Error is not recoverable: exiting now\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 148 ms, sys: 44.9 ms, total: 192 ms\n",
      "Wall time: 10.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Download the dataset\n",
    "!wget -P /tmp http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\n",
    "!wget -P /tmp http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar\n",
    "!wget -P /tmp http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar\n",
    "# # Extract the data.\n",
    "!tar -xf /tmp/VOCtrainval_11-May-2012.tar && rm /tmp/VOCtrainval_11-May-2012.tar\n",
    "!tar -xf /tmp/VOCtrainval_06-Nov-2007.tar && rm /tmp/VOCtrainval_06-Nov-2007.tar\n",
    "!tar -xf /tmp/VOCtest_06-Nov-2007.tar && rm /tmp/VOCtest_06-Nov-2007.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9255ea34",
   "metadata": {
    "papermill": {
     "duration": 0.028294,
     "end_time": "2021-06-08T00:15:37.501721",
     "exception": false,
     "start_time": "2021-06-08T00:15:37.473427",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Convert data into RecordIO\n",
    "[RecordIO](https://mxnet.incubator.apache.org/architecture/note_data_loading.html) is a highly efficient binary data format from [MXNet](https://mxnet.incubator.apache.org/) that makes it easy and simple to prepare the dataset and transfer to the instance that will run the training job. To generate a RecordIO file, we will use the tools from MXNet. The provided tools will first generate a list file and then use the [im2rec tool](https://github.com/apache/incubator-mxnet/blob/master/tools/im2rec.py) to create the [RecordIO](https://mxnet.incubator.apache.org/architecture/note_data_loading.html) file. More details on how to generate RecordIO file for object detection task, see the [MXNet example](https://github.com/apache/incubator-mxnet/tree/master/example/ssd).\n",
    "\n",
    "We will combine the training and validation sets from both 2007 and 2012 as the training data set, and use the test set from 2007 as our validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "639e515a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T00:15:37.566629Z",
     "iopub.status.busy": "2021-06-08T00:15:37.565856Z",
     "iopub.status.idle": "2021-06-08T00:15:38.528707Z",
     "shell.execute_reply": "2021-06-08T00:15:38.528185Z"
    },
    "papermill": {
     "duration": 0.998838,
     "end_time": "2021-06-08T00:15:38.528833",
     "exception": false,
     "start_time": "2021-06-08T00:15:37.529995",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"tools/prepare_dataset.py\", line 31, in <module>\r\n",
      "    from pascal_voc import PascalVoc\r\n",
      "  File \"/opt/ml/processing/input/tools/pascal_voc.py\", line 23, in <module>\r\n",
      "    import cv2\r\n",
      "ModuleNotFoundError: No module named 'cv2'\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"tools/prepare_dataset.py\", line 31, in <module>\r\n",
      "    from pascal_voc import PascalVoc\r\n",
      "  File \"/opt/ml/processing/input/tools/pascal_voc.py\", line 23, in <module>\r\n",
      "    import cv2\r\n",
      "ModuleNotFoundError: No module named 'cv2'\r\n"
     ]
    }
   ],
   "source": [
    "!python tools/prepare_dataset.py --dataset pascal --year 2007,2012 --set trainval --target VOCdevkit/train.lst\n",
    "!rm -rf VOCdevkit/VOC2012\n",
    "!python tools/prepare_dataset.py --dataset pascal --year 2007 --set test --target VOCdevkit/val.lst --no-shuffle\n",
    "!rm -rf VOCdevkit/VOC2007"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2976a59c",
   "metadata": {
    "papermill": {
     "duration": 0.028979,
     "end_time": "2021-06-08T00:15:38.586890",
     "exception": false,
     "start_time": "2021-06-08T00:15:38.557911",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Along with this notebook, we have provided tools that can directly generated the RecordIO files so that you do not need to do addtional work. These tools work with the Pascal datasets lst format, which is also quite the common among most datasets. If your data are stored in a different format or the annotation of your data is in a different format than the Pascal VOC dataset, you can also create the RecordIO by first generating the .lst file and then using the im2rec tool provided by MXNet. To make things clear, we will explain the definition of a .lst file so that you can prepare it in your own way. The following example is the first three lines of the .lst file we just generated for the Pascal VOC dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7e0b379",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T00:15:38.650656Z",
     "iopub.status.busy": "2021-06-08T00:15:38.649851Z",
     "iopub.status.idle": "2021-06-08T00:15:38.802400Z",
     "shell.execute_reply": "2021-06-08T00:15:38.802928Z"
    },
    "papermill": {
     "duration": 0.187171,
     "end_time": "2021-06-08T00:15:38.803089",
     "exception": false,
     "start_time": "2021-06-08T00:15:38.615918",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head: cannot open 'VOCdevkit/train.lst' for reading: No such file or directory\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "!head -n 3 VOCdevkit/train.lst > example.lst\n",
    "f = open(\"example.lst\", \"r\")\n",
    "lst_content = f.read()\n",
    "print(lst_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ccb1db",
   "metadata": {
    "papermill": {
     "duration": 0.029646,
     "end_time": "2021-06-08T00:15:38.862461",
     "exception": false,
     "start_time": "2021-06-08T00:15:38.832815",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "As can be seen that each line in the .lst file represents the annotations for a image. A .lst file is a **tab**-delimited file with multiple columns. The rows of the file are annotations of the of image files. The first column specifies a unique image index. The second column specifies the header size of the current row. In the above example .lst file, 2 from the second column means the second and third columns are header information, which will not be considered as label and bounding box information of the image specified by the current row.\n",
    "\n",
    "The third column specifies the label width of a single object. In the first row of above sample .lst file, 5 from the third row means each object within an image will have 5 numbers to describe its label information, including class index, and bounding box coordinates. If there are multiple objects within one image, all the label information should be listed in one line. The annotation information for each object is represented as ``[class_index, xmin, ymin, xmax, ymax]``. \n",
    "\n",
    "The classes should be labeled with successive numbers and start with 0. The bounding box coordinates are ratios of its top-left (xmin, ymin) and bottom-right (xmax, ymax) corner indices to the overall image size. Note that the top-left corner of the entire image is the origin (0, 0). The last column specifies the relative path of the image file.\n",
    "\n",
    "After generating the .lst file, the RecordIO can be created by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7233c589",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T00:15:38.926488Z",
     "iopub.status.busy": "2021-06-08T00:15:38.925654Z",
     "iopub.status.idle": "2021-06-08T00:15:38.927713Z",
     "shell.execute_reply": "2021-06-08T00:15:38.928142Z"
    },
    "papermill": {
     "duration": 0.036123,
     "end_time": "2021-06-08T00:15:38.928280",
     "exception": false,
     "start_time": "2021-06-08T00:15:38.892157",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# python /tools/im2rec.py --pack-label --num-thread 4 your_lst_file_name /your_image_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5025a6d7",
   "metadata": {
    "papermill": {
     "duration": 0.029696,
     "end_time": "2021-06-08T00:15:38.987688",
     "exception": false,
     "start_time": "2021-06-08T00:15:38.957992",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Upload data to S3\n",
    "Upload the data to the S3 bucket. We do this in multiple channels. Channels are simply directories in the bucket that differentiate between training and validation data. Let us simply call these directories `train` and `validation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c196065",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T00:15:39.055585Z",
     "iopub.status.busy": "2021-06-08T00:15:39.055023Z",
     "iopub.status.idle": "2021-06-08T00:15:39.175593Z",
     "shell.execute_reply": "2021-06-08T00:15:39.175082Z"
    },
    "papermill": {
     "duration": 0.158302,
     "end_time": "2021-06-08T00:15:39.175724",
     "exception": false,
     "start_time": "2021-06-08T00:15:39.017422",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'VOCdevkit/train.rec'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mupload_data\u001b[0;34m(self, path, bucket, key_prefix, extra_args)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlocal_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms3_key\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m             \u001b[0ms3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms3_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExtraArgs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextra_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0ms3_uri\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"s3://{}/{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/boto3/s3/inject.py\u001b[0m in \u001b[0;36mobject_upload_file\u001b[0;34m(self, Filename, ExtraArgs, Callback, Config)\u001b[0m\n\u001b[1;32m    278\u001b[0m     return self.meta.client.upload_file(\n\u001b[1;32m    279\u001b[0m         \u001b[0mFilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBucket\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbucket_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m         ExtraArgs=ExtraArgs, Callback=Callback, Config=Config)\n\u001b[0m\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/boto3/s3/inject.py\u001b[0m in \u001b[0;36mupload_file\u001b[0;34m(self, Filename, Bucket, Key, ExtraArgs, Callback, Config)\u001b[0m\n\u001b[1;32m    129\u001b[0m         return transfer.upload_file(\n\u001b[1;32m    130\u001b[0m             \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbucket\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBucket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mKey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             extra_args=ExtraArgs, callback=Callback)\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/boto3/s3/transfer.py\u001b[0m in \u001b[0;36mupload_file\u001b[0;34m(self, filename, bucket, key, callback, extra_args)\u001b[0m\n\u001b[1;32m    277\u001b[0m             filename, bucket, key, extra_args, subscribers)\n\u001b[1;32m    278\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0;31m# If a client error was raised, add the backwards compatibility layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;31m# that raises a S3UploadFailedError. These specific errors were only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/s3transfer/futures.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0;31m# however if a KeyboardInterrupt is raised we want want to exit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;31m# out of this and propogate the exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_coordinator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/s3transfer/futures.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;31m# final result.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/s3transfer/tasks.py\u001b[0m in \u001b[0;36m_main\u001b[0;34m(self, transfer_future, **kwargs)\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0;31m# Call the submit method to start submitting tasks to execute the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0;31m# transfer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_submit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransfer_future\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransfer_future\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0;31m# If there was an exception raised during the submission of task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/s3transfer/upload.py\u001b[0m in \u001b[0;36m_submit\u001b[0;34m(self, client, config, osutil, request_executor, transfer_future, bandwidth_limiter)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;31m# Determine the size if it was not provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtransfer_future\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m             \u001b[0mupload_input_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprovide_transfer_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransfer_future\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[0;31m# Do a multipart upload if needed, otherwise do a regular put object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/s3transfer/upload.py\u001b[0m in \u001b[0;36mprovide_transfer_size\u001b[0;34m(self, transfer_future)\u001b[0m\n\u001b[1;32m    235\u001b[0m         transfer_future.meta.provide_transfer_size(\n\u001b[1;32m    236\u001b[0m             self._osutil.get_file_size(\n\u001b[0;32m--> 237\u001b[0;31m                 transfer_future.meta.call_args.fileobj))\n\u001b[0m\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrequires_multipart_upload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransfer_future\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/s3transfer/utils.py\u001b[0m in \u001b[0;36mget_file_size\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_file_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mopen_file_chunk_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_byte\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/genericpath.py\u001b[0m in \u001b[0;36mgetsize\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgetsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;34m\"\"\"Return the size of a file, reported by os.stat().\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mst_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'VOCdevkit/train.rec'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Upload the RecordIO files to train and validation channels\n",
    "train_channel = prefix + \"/train\"\n",
    "validation_channel = prefix + \"/validation\"\n",
    "\n",
    "s3_train_data = \"s3://{}/{}\".format(bucket, train_channel)\n",
    "s3_validation_data = \"s3://{}/{}\".format(bucket, validation_channel)\n",
    "\n",
    "sess.upload_data(path=\"VOCdevkit/train.rec\", bucket=bucket, key_prefix=train_channel)\n",
    "sess.upload_data(path=\"VOCdevkit/val.rec\", bucket=bucket, key_prefix=validation_channel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54da626c",
   "metadata": {
    "papermill": {
     "duration": 0.030348,
     "end_time": "2021-06-08T00:15:39.236662",
     "exception": false,
     "start_time": "2021-06-08T00:15:39.206314",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Next we need to setup an output location at S3, where the model artifact will be dumped. These artifacts are also the output of the algorithm's traning job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbc3e702",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T00:15:39.301416Z",
     "iopub.status.busy": "2021-06-08T00:15:39.300588Z",
     "iopub.status.idle": "2021-06-08T00:15:39.303248Z",
     "shell.execute_reply": "2021-06-08T00:15:39.302660Z"
    },
    "papermill": {
     "duration": 0.036536,
     "end_time": "2021-06-08T00:15:39.303373",
     "exception": false,
     "start_time": "2021-06-08T00:15:39.266837",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_output_location = \"s3://{}/{}/output\".format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12fa339",
   "metadata": {
    "papermill": {
     "duration": 0.030627,
     "end_time": "2021-06-08T00:15:39.364741",
     "exception": false,
     "start_time": "2021-06-08T00:15:39.334114",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Initial Training\n",
    "Now that we are done with all the setup that is needed, we are ready to train our object detector. To begin, let us create a ``sageMaker.estimator.Estimator`` object. This estimator will launch the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "761aa51a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T00:15:39.431119Z",
     "iopub.status.busy": "2021-06-08T00:15:39.430200Z",
     "iopub.status.idle": "2021-06-08T00:15:39.435917Z",
     "shell.execute_reply": "2021-06-08T00:15:39.436338Z"
    },
    "papermill": {
     "duration": 0.041021,
     "end_time": "2021-06-08T00:15:39.436483",
     "exception": false,
     "start_time": "2021-06-08T00:15:39.395462",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_instance_count has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_max_run has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_volume_size has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "od_model = sagemaker.estimator.Estimator(\n",
    "    training_image,\n",
    "    role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type=\"ml.p3.2xlarge\",\n",
    "    train_volume_size=50,\n",
    "    train_max_run=360000,\n",
    "    input_mode=\"File\",\n",
    "    output_path=s3_output_location,\n",
    "    sagemaker_session=sess,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa38f711",
   "metadata": {
    "papermill": {
     "duration": 0.031626,
     "end_time": "2021-06-08T00:15:39.499772",
     "exception": false,
     "start_time": "2021-06-08T00:15:39.468146",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The object detection algorithm at its core is the [Single-Shot Multi-Box detection algorithm (SSD)](https://arxiv.org/abs/1512.02325). This algorithm uses a `base_network`, which is typically a [VGG](https://arxiv.org/abs/1409.1556) or a [ResNet](https://arxiv.org/abs/1512.03385). The Amazon SageMaker object detection algorithm supports VGG-16 and ResNet-50 now. It also has a lot of options for hyperparameters that help configure the training job. The next step in our training, is to setup these hyperparameters and data channels for training the model. Consider the following example definition of hyperparameters. See the SageMaker Object Detection [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/object-detection.html) for more details on the hyperparameters.\n",
    "\n",
    "One of the hyperparameters here for instance is the `epochs`. This defines how many passes of the dataset we iterate over and determines that training time of the algorithm. In this example, we train the model for `5` epochs to generate a basic model for the PASCAL VOC dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ebd79d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T00:15:39.568228Z",
     "iopub.status.busy": "2021-06-08T00:15:39.567667Z",
     "iopub.status.idle": "2021-06-08T00:15:39.570013Z",
     "shell.execute_reply": "2021-06-08T00:15:39.569552Z"
    },
    "papermill": {
     "duration": 0.038686,
     "end_time": "2021-06-08T00:15:39.570132",
     "exception": false,
     "start_time": "2021-06-08T00:15:39.531446",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "od_model.set_hyperparameters(\n",
    "    base_network=\"resnet-50\",\n",
    "    use_pretrained_model=1,\n",
    "    num_classes=20,\n",
    "    mini_batch_size=16,\n",
    "    epochs=5,\n",
    "    learning_rate=0.001,\n",
    "    lr_scheduler_step=\"3,6\",\n",
    "    lr_scheduler_factor=0.1,\n",
    "    optimizer=\"rmsprop\",\n",
    "    momentum=0.9,\n",
    "    weight_decay=0.0005,\n",
    "    overlap_threshold=0.5,\n",
    "    nms_threshold=0.45,\n",
    "    image_shape=300,\n",
    "    label_width=350,\n",
    "    num_training_samples=16551,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a60639",
   "metadata": {
    "papermill": {
     "duration": 0.031548,
     "end_time": "2021-06-08T00:15:39.633606",
     "exception": false,
     "start_time": "2021-06-08T00:15:39.602058",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now that the hyperparameters are setup, let us prepare the handshake between our data channels and the algorithm. To do this, we need to create the `sagemaker.session.s3_input` objects from our data channels. These objects are then put in a simple dictionary, which the algorithm consumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "256af7dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T00:15:39.702261Z",
     "iopub.status.busy": "2021-06-08T00:15:39.701691Z",
     "iopub.status.idle": "2021-06-08T00:15:39.705874Z",
     "shell.execute_reply": "2021-06-08T00:15:39.705410Z"
    },
    "papermill": {
     "duration": 0.040736,
     "end_time": "2021-06-08T00:15:39.705993",
     "exception": false,
     "start_time": "2021-06-08T00:15:39.665257",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The class sagemaker.session.s3_input has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The class sagemaker.session.s3_input has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "train_data = sagemaker.session.s3_input(\n",
    "    s3_train_data,\n",
    "    distribution=\"FullyReplicated\",\n",
    "    content_type=\"application/x-recordio\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    ")\n",
    "validation_data = sagemaker.session.s3_input(\n",
    "    s3_validation_data,\n",
    "    distribution=\"FullyReplicated\",\n",
    "    content_type=\"application/x-recordio\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    ")\n",
    "data_channels = {\"train\": train_data, \"validation\": validation_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0428450e",
   "metadata": {
    "papermill": {
     "duration": 0.032772,
     "end_time": "2021-06-08T00:15:39.771720",
     "exception": false,
     "start_time": "2021-06-08T00:15:39.738948",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We have our `Estimator` object, we have set the hyperparameters for this object and we have our data channels linked with the algorithm. The only remaining thing to do is to train the algorithm. The following command will train the algorithm. Training the algorithm involves a few steps. Firstly, the instances that we requested while creating the `Estimator` classes are provisioned and are setup with the appropriate libraries. Then, the data from our channels are downloaded into the instance. Once this is done, the training job begins. The provisioning and data downloading will take time, depending on the size of the data. Therefore it might be a few minutes before we start getting data logs for our training jobs. The data logs will also print out Mean Average Precision (mAP) on the validation data, among other losses, for every run of the dataset once or one epoch. This metric is a proxy for the quality of the algorithm. \n",
    "\n",
    "Once the job has finished a \"Training job completed\" message will be printed. The trained model can be found in the S3 bucket that was setup as `output_path` in the estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1fce6260",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T00:15:39.841686Z",
     "iopub.status.busy": "2021-06-08T00:15:39.840795Z",
     "iopub.status.idle": "2021-06-08T00:41:37.781966Z",
     "shell.execute_reply": "2021-06-08T00:41:37.781456Z"
    },
    "papermill": {
     "duration": 1557.977819,
     "end_time": "2021-06-08T00:41:37.782094",
     "exception": false,
     "start_time": "2021-06-08T00:15:39.804275",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-08 00:15:40 Starting - Starting the training job."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2021-06-08 00:16:06 Starting - Launching requested ML instancesProfilerReport-1623111339: InProgress\n",
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2021-06-08 00:17:07 Starting - Preparing the instances for training."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2021-06-08 00:18:34 Downloading - Downloading input data."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2021-06-08 00:19:27 Training - Downloading the training image."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2021-06-08 00:20:11 Training - Training image download completed. Training in progress.."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:20:14 INFO 139763492013888] Reading default configuration from /opt/amazon/lib/python3.7/site-packages/algorithm/default-input.json: {'base_network': 'vgg-16', 'use_pretrained_model': '0', 'num_classes': '', 'mini_batch_size': '32', 'epochs': '30', 'learning_rate': '0.001', 'lr_scheduler_step': '', 'lr_scheduler_factor': '0.1', 'optimizer': 'sgd', 'momentum': '0.9', 'weight_decay': '0.0005', 'overlap_threshold': '0.5', 'nms_threshold': '0.45', 'num_training_samples': '', 'image_shape': '300', '_tuning_objective_metric': '', '_kvstore': 'device', 'kv_store': 'device', '_num_kv_servers': 'auto', 'label_width': '350', 'freeze_layer_pattern': '', 'nms_topk': '400', 'early_stopping': 'False', 'early_stopping_min_epochs': '10', 'early_stopping_patience': '5', 'early_stopping_tolerance': '0.0', '_begin_epoch': '0'}\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:20:14 INFO 139763492013888] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'weight_decay': '0.0005', 'num_classes': '20', 'num_training_samples': '16551', 'lr_scheduler_step': '3,6', 'overlap_threshold': '0.5', 'image_shape': '300', 'label_width': '350', 'momentum': '0.9', 'nms_threshold': '0.45', 'lr_scheduler_factor': '0.1', 'optimizer': 'rmsprop', 'base_network': 'resnet-50', 'use_pretrained_model': '1', 'epochs': '5', 'learning_rate': '0.001', 'mini_batch_size': '16'}\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:20:14 INFO 139763492013888] Final configuration: {'base_network': 'resnet-50', 'use_pretrained_model': '1', 'num_classes': '20', 'mini_batch_size': '16', 'epochs': '5', 'learning_rate': '0.001', 'lr_scheduler_step': '3,6', 'lr_scheduler_factor': '0.1', 'optimizer': 'rmsprop', 'momentum': '0.9', 'weight_decay': '0.0005', 'overlap_threshold': '0.5', 'nms_threshold': '0.45', 'num_training_samples': '16551', 'image_shape': '300', '_tuning_objective_metric': '', '_kvstore': 'device', 'kv_store': 'device', '_num_kv_servers': 'auto', 'label_width': '350', 'freeze_layer_pattern': '', 'nms_topk': '400', 'early_stopping': 'False', 'early_stopping_min_epochs': '10', 'early_stopping_patience': '5', 'early_stopping_tolerance': '0.0', '_begin_epoch': '0'}\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:20:14 INFO 139763492013888] Using default worker.\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:20:14 INFO 139763492013888] Loaded iterator creator application/x-image for content type ('application/x-image', '1.0')\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:20:14 INFO 139763492013888] Loaded iterator creator application/x-recordio for content type ('application/x-recordio', '1.0')\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:20:14 INFO 139763492013888] Loaded iterator creator image/jpeg for content type ('image/jpeg', '1.0')\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:20:14 INFO 139763492013888] Loaded iterator creator image/png for content type ('image/png', '1.0')\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:20:14 INFO 139763492013888] Checkpoint loading and saving are disabled.\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:20:14 INFO 139763492013888] nvidia-smi: took 0.106 seconds to run.\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:20:14 INFO 139763492013888] nvidia-smi identified 1 GPUs.\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:20:14 INFO 139763492013888] Number of GPUs being used: 1\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:20:14 WARNING 139763492013888] Training images are resized to image shape (3, 300, 300)\u001b[0m\n",
      "\u001b[34m[00:20:14] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.4.x_ecl_Cuda_9.x.135.0/AL2_x86_64/generic-flavor/src/src/io/iter_image_det_recordio.cc:283: ImageDetRecordIOParser: /opt/ml/input/data/train/train.rec, use 7 threads for decoding..\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[00:20:22] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.4.x_ecl_Cuda_9.x.135.0/AL2_x86_64/generic-flavor/src/src/io/iter_image_det_recordio.cc:340: ImageDetRecordIOParser: /opt/ml/input/data/train/train.rec, label padding width: 350\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:20:23 WARNING 139763492013888] Validation images are resized to image shape (3, 300, 300)\u001b[0m\n",
      "\u001b[34m[00:20:23] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.4.x_ecl_Cuda_9.x.135.0/AL2_x86_64/generic-flavor/src/src/io/iter_image_det_recordio.cc:283: ImageDetRecordIOParser: /opt/ml/input/data/validation/val.rec, use 7 threads for decoding..\u001b[0m\n",
      "\u001b[34m[00:20:25] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.4.x_ecl_Cuda_9.x.135.0/AL2_x86_64/generic-flavor/src/src/io/iter_image_det_recordio.cc:340: ImageDetRecordIOParser: /opt/ml/input/data/validation/val.rec, label padding width: 350\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/08/2021 00:20:27 INFO 139763492013888] Number of GPUs being used: 1\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:20:27 INFO 139763492013888] Using [gpu(0)] as training context.\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:20:27 INFO 139763492013888] Number of GPUs being used: 1\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:20:27 INFO 139763492013888] Create Store: device\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:20:27 INFO 139763492013888] Using (gpu(0)) as training context.\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:20:27 INFO 139763492013888] Start training from pretrained model 1.\u001b[0m\n",
      "\u001b[34m[00:20:27] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.4.x_ecl_Cuda_9.x.135.0/AL2_x86_64/generic-flavor/src/src/nnvm/legacy_json_util.cc:209: Loading symbol saved by previous version v0.8.0. Attempting to upgrade...\u001b[0m\n",
      "\u001b[34m[00:20:27] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.4.x_ecl_Cuda_9.x.135.0/AL2_x86_64/generic-flavor/src/src/nnvm/legacy_json_util.cc:217: Symbol successfully upgraded!\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:20:28 INFO 139763492013888] Loaded pretrained model parameters.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/08/2021 00:20:39 INFO 139763492013888] Creating a new state instance.\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623111639.4975927, \"EndTime\": 1623111639.4976678, \"Dimensions\": {\"Algorithm\": \"AWS/Object Detection\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"Meta\": \"init_train_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Total Batches Seen\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Max Records Seen Between Resets\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Max Batches Seen Between Resets\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Reset Count\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Number of Records Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Number of Batches Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[00:20:39] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.4.x_ecl_Cuda_9.x.135.0/AL2_x86_64/generic-flavor/src/src/operator/nn/./cudnn/./cudnn_algoreg-inl.h:97: Running performance tests to find the best convolution algorithm, this can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/08/2021 00:21:01 INFO 139763492013888] Epoch:    0, batches:    100, num_examples:   1600, 72.5 samples/sec, epoch time so far:  0:00:22.067521\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/08/2021 00:21:19 INFO 139763492013888] Epoch:    0, batches:    200, num_examples:   3200, 80.4 samples/sec, epoch time so far:  0:00:39.817381\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/08/2021 00:21:36 INFO 139763492013888] Epoch:    0, batches:    300, num_examples:   4800, 84.1 samples/sec, epoch time so far:  0:00:57.089647\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/08/2021 00:21:53 INFO 139763492013888] Epoch:    0, batches:    400, num_examples:   6400, 86.1 samples/sec, epoch time so far:  0:01:14.296925\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/08/2021 00:22:12 INFO 139763492013888] Epoch:    0, batches:    500, num_examples:   8000, 85.8 samples/sec, epoch time so far:  0:01:33.250690\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/08/2021 00:22:30 INFO 139763492013888] Epoch:    0, batches:    600, num_examples:   9600, 86.8 samples/sec, epoch time so far:  0:01:50.537062\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/08/2021 00:22:47 INFO 139763492013888] Epoch:    0, batches:    700, num_examples:   11200, 87.6 samples/sec, epoch time so far:  0:02:07.825017\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/08/2021 00:23:04 INFO 139763492013888] Epoch:    0, batches:    800, num_examples:   12800, 88.4 samples/sec, epoch time so far:  0:02:24.791444\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/08/2021 00:23:22 INFO 139763492013888] Epoch:    0, batches:    900, num_examples:   14400, 88.3 samples/sec, epoch time so far:  0:02:43.149788\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/08/2021 00:23:39 INFO 139763492013888] Epoch:    0, batches:    1000, num_examples:   16000, 88.7 samples/sec, epoch time so far:  0:03:00.341104\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:23:45 INFO 139763492013888] #quality_metric: host=algo-1, epoch=0, batch=1035 train cross_entropy <loss>=(1.1592809385995073)\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:23:45 INFO 139763492013888] #quality_metric: host=algo-1, epoch=0, batch=1035 train smooth_l1 <loss>=(0.5428036072072824)\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:23:45 INFO 139763492013888] Round of batches complete\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:23:45 INFO 139763492013888] Updated the metrics\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/08/2021 00:24:43 INFO 139763492013888] #quality_metric: host=algo-1, epoch=0, validation mAP <score>=(0.19905460940354514)\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:24:43 INFO 139763492013888] Updating the best model with validation-mAP=0.19905460940354514\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:24:44 INFO 139763492013888] Saved checkpoint to \"/opt/ml/model/model_algo_1-0000.params\"\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:24:44 INFO 139763492013888] #progress_metric: host=algo-1, completed 20.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623111639.4980025, \"EndTime\": 1623111884.0310733, \"Dimensions\": {\"Algorithm\": \"AWS/Object Detection\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 0, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Total Batches Seen\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Max Records Seen Between Resets\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Max Batches Seen Between Resets\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Reset Count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Number of Records Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Number of Batches Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}}}\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/08/2021 00:25:02 INFO 139763492013888] Epoch:    1, batches:    100, num_examples:   1600, 89.0 samples/sec, epoch time so far:  0:00:17.982581\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/08/2021 00:25:19 INFO 139763492013888] Epoch:    1, batches:    200, num_examples:   3200, 89.1 samples/sec, epoch time so far:  0:00:35.902736\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/08/2021 00:25:37 INFO 139763492013888] Epoch:    1, batches:    300, num_examples:   4800, 90.2 samples/sec, epoch time so far:  0:00:53.222602\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/08/2021 00:25:54 INFO 139763492013888] Epoch:    1, batches:    400, num_examples:   6400, 90.2 samples/sec, epoch time so far:  0:01:10.919142\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/08/2021 00:26:12 INFO 139763492013888] Epoch:    1, batches:    500, num_examples:   8000, 90.5 samples/sec, epoch time so far:  0:01:28.444747\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/08/2021 00:26:29 INFO 139763492013888] Epoch:    1, batches:    600, num_examples:   9600, 90.7 samples/sec, epoch time so far:  0:01:45.897447\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/08/2021 00:26:47 INFO 139763492013888] Epoch:    1, batches:    700, num_examples:   11200, 90.6 samples/sec, epoch time so far:  0:02:03.603351\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/08/2021 00:27:05 INFO 139763492013888] Epoch:    1, batches:    800, num_examples:   12800, 90.7 samples/sec, epoch time so far:  0:02:21.101812\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/08/2021 00:27:23 INFO 139763492013888] Epoch:    1, batches:    900, num_examples:   14400, 90.5 samples/sec, epoch time so far:  0:02:39.187824\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/08/2021 00:27:40 INFO 139763492013888] Epoch:    1, batches:    1000, num_examples:   16000, 90.5 samples/sec, epoch time so far:  0:02:56.753671\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/08/2021 00:27:45 INFO 139763492013888] #quality_metric: host=algo-1, epoch=1, batch=1034 train cross_entropy <loss>=(0.9489656753341774)\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:27:45 INFO 139763492013888] #quality_metric: host=algo-1, epoch=1, batch=1034 train smooth_l1 <loss>=(0.43904152421094456)\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:27:45 INFO 139763492013888] Round of batches complete\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:27:46 INFO 139763492013888] Updated the metrics\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/08/2021 00:28:42 INFO 139763492013888] #quality_metric: host=algo-1, epoch=1, validation mAP <score>=(0.28848972282396856)\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:28:42 INFO 139763492013888] Updating the best model with validation-mAP=0.28848972282396856\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:28:43 INFO 139763492013888] Saved checkpoint to \"/opt/ml/model/model_algo_1-0000.params\"\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:28:43 INFO 139763492013888] #progress_metric: host=algo-1, completed 40.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623111884.0313184, \"EndTime\": 1623112123.1270182, \"Dimensions\": {\"Algorithm\": \"AWS/Object Detection\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 1, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Total Batches Seen\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Max Records Seen Between Resets\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Max Batches Seen Between Resets\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Reset Count\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Number of Records Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Number of Batches Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}}}\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/08/2021 00:29:01 INFO 139763492013888] Epoch:    2, batches:    100, num_examples:   1600, 89.1 samples/sec, epoch time so far:  0:00:17.954888\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/08/2021 00:29:18 INFO 139763492013888] Epoch:    2, batches:    200, num_examples:   3200, 89.5 samples/sec, epoch time so far:  0:00:35.745898\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/08/2021 00:29:36 INFO 139763492013888] Epoch:    2, batches:    300, num_examples:   4800, 90.3 samples/sec, epoch time so far:  0:00:53.139882\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/08/2021 00:29:53 INFO 139763492013888] Epoch:    2, batches:    400, num_examples:   6400, 90.9 samples/sec, epoch time so far:  0:01:10.410421\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/08/2021 00:30:11 INFO 139763492013888] Epoch:    2, batches:    500, num_examples:   8000, 90.5 samples/sec, epoch time so far:  0:01:28.351588\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/08/2021 00:30:28 INFO 139763492013888] Epoch:    2, batches:    600, num_examples:   9600, 90.7 samples/sec, epoch time so far:  0:01:45.795781\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/08/2021 00:30:46 INFO 139763492013888] Epoch:    2, batches:    700, num_examples:   11200, 90.9 samples/sec, epoch time so far:  0:02:03.230409\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/08/2021 00:31:03 INFO 139763492013888] Epoch:    2, batches:    800, num_examples:   12800, 90.9 samples/sec, epoch time so far:  0:02:20.871359\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/08/2021 00:31:21 INFO 139763492013888] Epoch:    2, batches:    900, num_examples:   14400, 90.8 samples/sec, epoch time so far:  0:02:38.557186\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/08/2021 00:31:38 INFO 139763492013888] Epoch:    2, batches:    1000, num_examples:   16000, 91.0 samples/sec, epoch time so far:  0:02:55.769493\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/08/2021 00:31:44 INFO 139763492013888] Update[3103]: Change learning rate to 1.00000e-05\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:31:44 INFO 139763492013888] #quality_metric: host=algo-1, epoch=2, batch=1035 train cross_entropy <loss>=(0.8826827490410961)\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:31:44 INFO 139763492013888] #quality_metric: host=algo-1, epoch=2, batch=1035 train smooth_l1 <loss>=(0.4071018180537036)\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:31:44 INFO 139763492013888] Round of batches complete\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:31:44 INFO 139763492013888] Updated the metrics\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/08/2021 00:32:43 INFO 139763492013888] #quality_metric: host=algo-1, epoch=2, validation mAP <score>=(0.3345820201721826)\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:32:43 INFO 139763492013888] Updating the best model with validation-mAP=0.3345820201721826\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:32:43 INFO 139763492013888] Saved checkpoint to \"/opt/ml/model/model_algo_1-0000.params\"\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:32:43 INFO 139763492013888] #progress_metric: host=algo-1, completed 60.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623112123.127278, \"EndTime\": 1623112363.3974922, \"Dimensions\": {\"Algorithm\": \"AWS/Object Detection\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 2, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Total Batches Seen\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Max Records Seen Between Resets\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Max Batches Seen Between Resets\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Reset Count\": {\"sum\": 3.0, \"count\": 1, \"min\": 3, \"max\": 3}, \"Number of Records Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Number of Batches Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}}}\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/08/2021 00:33:01 INFO 139763492013888] Epoch:    3, batches:    100, num_examples:   1600, 89.9 samples/sec, epoch time so far:  0:00:17.799003\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/08/2021 00:33:19 INFO 139763492013888] Epoch:    3, batches:    200, num_examples:   3200, 89.7 samples/sec, epoch time so far:  0:00:35.672482\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/08/2021 00:33:36 INFO 139763492013888] Epoch:    3, batches:    300, num_examples:   4800, 90.3 samples/sec, epoch time so far:  0:00:53.139785\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/08/2021 00:33:53 INFO 139763492013888] Epoch:    3, batches:    400, num_examples:   6400, 90.7 samples/sec, epoch time so far:  0:01:10.572073\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/08/2021 00:34:11 INFO 139763492013888] Epoch:    3, batches:    500, num_examples:   8000, 90.8 samples/sec, epoch time so far:  0:01:28.143912\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/08/2021 00:34:28 INFO 139763492013888] Epoch:    3, batches:    600, num_examples:   9600, 91.1 samples/sec, epoch time so far:  0:01:45.413063\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/08/2021 00:34:46 INFO 139763492013888] Epoch:    3, batches:    700, num_examples:   11200, 91.3 samples/sec, epoch time so far:  0:02:02.620887\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/08/2021 00:35:03 INFO 139763492013888] Epoch:    3, batches:    800, num_examples:   12800, 91.5 samples/sec, epoch time so far:  0:02:19.815928\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/08/2021 00:35:22 INFO 139763492013888] Epoch:    3, batches:    900, num_examples:   14400, 90.8 samples/sec, epoch time so far:  0:02:38.660342\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/08/2021 00:35:39 INFO 139763492013888] Epoch:    3, batches:    1000, num_examples:   16000, 90.9 samples/sec, epoch time so far:  0:02:55.955788\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:35:44 INFO 139763492013888] #quality_metric: host=algo-1, epoch=3, batch=1034 train cross_entropy <loss>=(0.7979718979717247)\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:35:44 INFO 139763492013888] #quality_metric: host=algo-1, epoch=3, batch=1034 train smooth_l1 <loss>=(0.3725223473486877)\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:35:44 INFO 139763492013888] Round of batches complete\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:35:44 INFO 139763492013888] Updated the metrics\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/08/2021 00:36:37 INFO 139763492013888] #quality_metric: host=algo-1, epoch=3, validation mAP <score>=(0.4314690367406807)\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:36:37 INFO 139763492013888] Updating the best model with validation-mAP=0.4314690367406807\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:36:37 INFO 139763492013888] Saved checkpoint to \"/opt/ml/model/model_algo_1-0000.params\"\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:36:37 INFO 139763492013888] #progress_metric: host=algo-1, completed 80.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623112363.3977618, \"EndTime\": 1623112597.6371493, \"Dimensions\": {\"Algorithm\": \"AWS/Object Detection\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 3, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Total Batches Seen\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Max Records Seen Between Resets\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Max Batches Seen Between Resets\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Reset Count\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Number of Records Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Number of Batches Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}}}\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/08/2021 00:36:55 INFO 139763492013888] Epoch:    4, batches:    100, num_examples:   1600, 91.8 samples/sec, epoch time so far:  0:00:17.422129\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/08/2021 00:37:12 INFO 139763492013888] Epoch:    4, batches:    200, num_examples:   3200, 90.9 samples/sec, epoch time so far:  0:00:35.190087\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/08/2021 00:37:30 INFO 139763492013888] Epoch:    4, batches:    300, num_examples:   4800, 91.3 samples/sec, epoch time so far:  0:00:52.573873\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/08/2021 00:37:47 INFO 139763492013888] Epoch:    4, batches:    400, num_examples:   6400, 91.7 samples/sec, epoch time so far:  0:01:09.794477\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/08/2021 00:38:04 INFO 139763492013888] Epoch:    4, batches:    500, num_examples:   8000, 91.9 samples/sec, epoch time so far:  0:01:27.022665\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/08/2021 00:38:22 INFO 139763492013888] Epoch:    4, batches:    600, num_examples:   9600, 91.9 samples/sec, epoch time so far:  0:01:44.491156\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/08/2021 00:38:39 INFO 139763492013888] Epoch:    4, batches:    700, num_examples:   11200, 92.0 samples/sec, epoch time so far:  0:02:01.728222\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/08/2021 00:38:56 INFO 139763492013888] Epoch:    4, batches:    800, num_examples:   12800, 92.1 samples/sec, epoch time so far:  0:02:18.996809\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/08/2021 00:39:14 INFO 139763492013888] Epoch:    4, batches:    900, num_examples:   14400, 91.9 samples/sec, epoch time so far:  0:02:36.644035\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/08/2021 00:39:31 INFO 139763492013888] Epoch:    4, batches:    1000, num_examples:   16000, 92.1 samples/sec, epoch time so far:  0:02:53.722785\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:39:36 INFO 139763492013888] #quality_metric: host=algo-1, epoch=4, batch=1035 train cross_entropy <loss>=(0.7741237876253803)\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:39:36 INFO 139763492013888] #quality_metric: host=algo-1, epoch=4, batch=1035 train smooth_l1 <loss>=(0.3593430273048518)\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:39:36 INFO 139763492013888] Round of batches complete\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:39:36 INFO 139763492013888] Updated the metrics\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/08/2021 00:40:31 INFO 139763492013888] #quality_metric: host=algo-1, epoch=4, validation mAP <score>=(0.4523574392628644)\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:40:31 INFO 139763492013888] Updating the best model with validation-mAP=0.4523574392628644\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:40:31 INFO 139763492013888] Saved checkpoint to \"/opt/ml/model/model_algo_1-0000.params\"\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:40:31 INFO 139763492013888] #progress_metric: host=algo-1, completed 100.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623112597.6374252, \"EndTime\": 1623112831.2886467, \"Dimensions\": {\"Algorithm\": \"AWS/Object Detection\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 4, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Total Batches Seen\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Max Records Seen Between Resets\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Max Batches Seen Between Resets\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Reset Count\": {\"sum\": 5.0, \"count\": 1, \"min\": 5, \"max\": 5}, \"Number of Records Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Number of Batches Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:40:31 WARNING 139763492013888] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:40:31 INFO 139763492013888] Saved checkpoint to \"/opt/ml/model/model_algo_1-0000.params\"\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:40:31 INFO 139763492013888] Test data is not provided.\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623111614.8264358, \"EndTime\": 1623112831.965931, \"Dimensions\": {\"Algorithm\": \"AWS/Object Detection\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"epochs\": {\"sum\": 5.0, \"count\": 1, \"min\": 5, \"max\": 5}, \"setuptime\": {\"sum\": 7.905721664428711, \"count\": 1, \"min\": 7.905721664428711, \"max\": 7.905721664428711}, \"totaltime\": {\"sum\": 1217250.352859497, \"count\": 1, \"min\": 1217250.352859497, \"max\": 1217250.352859497}}}\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2021-06-08 00:40:52 Uploading - Uploading generated training model"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2021-06-08 00:41:12 Completed - Training job completed\n",
      "ProfilerReport-1623111339: IssuesFound\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training seconds: 1343\n",
      "Billable seconds: 1343\n"
     ]
    }
   ],
   "source": [
    "od_model.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe1f5ba",
   "metadata": {
    "papermill": {
     "duration": 0.05671,
     "end_time": "2021-06-08T00:41:37.895724",
     "exception": false,
     "start_time": "2021-06-08T00:41:37.839014",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "As you can see that it took about `18` minutes to reach a mAP around `0.4`. To improve the detection quality, you can start a new training job with an increased `epochs` to let the algorithm training for more iterations. However, the new training job will re-learn everything you have learned with the previous training job in the first `5` epochs. To avoid wasting the training resources and time, we can start the new training with a model that was generated in the previous SageMaker training jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc84e077",
   "metadata": {
    "papermill": {
     "duration": 0.058729,
     "end_time": "2021-06-08T00:41:38.010778",
     "exception": false,
     "start_time": "2021-06-08T00:41:37.952049",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Incremental Training\n",
    "In this section, we start a new training job from the model obtained in previous section. We setup the estimator and hyperparameters similar to the previous training job. Note that SageMaker object detection algorithm currently only support the re-training feature with the same network, which means the new training job must have the same `base_network` and `num_classes` as the previous training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c03c972",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T00:41:38.129575Z",
     "iopub.status.busy": "2021-06-08T00:41:38.128876Z",
     "iopub.status.idle": "2021-06-08T00:41:38.134354Z",
     "shell.execute_reply": "2021-06-08T00:41:38.134800Z"
    },
    "papermill": {
     "duration": 0.066517,
     "end_time": "2021-06-08T00:41:38.134944",
     "exception": false,
     "start_time": "2021-06-08T00:41:38.068427",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_instance_count has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_max_run has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_volume_size has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "new_od_model = sagemaker.estimator.Estimator(\n",
    "    training_image,\n",
    "    role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type=\"ml.p3.2xlarge\",\n",
    "    train_volume_size=50,\n",
    "    train_max_run=360000,\n",
    "    input_mode=\"File\",\n",
    "    output_path=s3_output_location,\n",
    "    sagemaker_session=sess,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2abf7c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T00:41:38.255415Z",
     "iopub.status.busy": "2021-06-08T00:41:38.254594Z",
     "iopub.status.idle": "2021-06-08T00:41:38.256720Z",
     "shell.execute_reply": "2021-06-08T00:41:38.257151Z"
    },
    "papermill": {
     "duration": 0.064732,
     "end_time": "2021-06-08T00:41:38.257289",
     "exception": false,
     "start_time": "2021-06-08T00:41:38.192557",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_od_model.set_hyperparameters(\n",
    "    base_network=\"resnet-50\",\n",
    "    num_classes=20,\n",
    "    mini_batch_size=16,\n",
    "    epochs=1,\n",
    "    learning_rate=0.001,\n",
    "    optimizer=\"rmsprop\",\n",
    "    momentum=0.9,\n",
    "    image_shape=300,\n",
    "    label_width=350,\n",
    "    num_training_samples=16551,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e02b40",
   "metadata": {
    "papermill": {
     "duration": 0.057846,
     "end_time": "2021-06-08T00:41:38.373265",
     "exception": false,
     "start_time": "2021-06-08T00:41:38.315419",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We use the same training data from previous job. To use the pre-trained model, we just need to add a `model` channel to the `inputs` and set its content type to `application/x-sagemaker-model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "861ed42d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T00:41:38.494367Z",
     "iopub.status.busy": "2021-06-08T00:41:38.493740Z",
     "iopub.status.idle": "2021-06-08T00:41:38.546787Z",
     "shell.execute_reply": "2021-06-08T00:41:38.547246Z"
    },
    "papermill": {
     "duration": 0.116477,
     "end_time": "2021-06-08T00:41:38.547396",
     "exception": false,
     "start_time": "2021-06-08T00:41:38.430919",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The class sagemaker.session.s3_input has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The class sagemaker.session.s3_input has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The class sagemaker.session.s3_input has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "# Use the same data for training and validation as the previous job.\n",
    "train_data = sagemaker.session.s3_input(\n",
    "    s3_train_data,\n",
    "    distribution=\"FullyReplicated\",\n",
    "    content_type=\"application/x-recordio\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    ")\n",
    "validation_data = sagemaker.session.s3_input(\n",
    "    s3_validation_data,\n",
    "    distribution=\"FullyReplicated\",\n",
    "    content_type=\"application/x-recordio\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    ")\n",
    "\n",
    "# Use the output model from the previous job.\n",
    "s3_model_data = od_model.model_data\n",
    "\n",
    "model_data = sagemaker.session.s3_input(\n",
    "    s3_model_data,\n",
    "    distribution=\"FullyReplicated\",\n",
    "    content_type=\"application/x-sagemaker-model\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    ")\n",
    "\n",
    "# In addition to two data channels, add a 'model' channel for the training.\n",
    "new_data_channels = {\"train\": train_data, \"validation\": validation_data, \"model\": model_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4ed55a",
   "metadata": {
    "papermill": {
     "duration": 0.058313,
     "end_time": "2021-06-08T00:41:38.664295",
     "exception": false,
     "start_time": "2021-06-08T00:41:38.605982",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Fit the new model with all three input channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ef31260",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T00:41:38.785483Z",
     "iopub.status.busy": "2021-06-08T00:41:38.784922Z",
     "iopub.status.idle": "2021-06-08T00:51:24.532178Z",
     "shell.execute_reply": "2021-06-08T00:51:24.532638Z"
    },
    "papermill": {
     "duration": 585.810134,
     "end_time": "2021-06-08T00:51:24.532811",
     "exception": false,
     "start_time": "2021-06-08T00:41:38.722677",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-08 00:41:39 Starting - Starting the training job."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2021-06-08 00:42:03 Starting - Launching requested ML instancesProfilerReport-1623112898: InProgress\n",
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2021-06-08 00:43:04 Starting - Preparing the instances for training."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2021-06-08 00:44:34 Downloading - Downloading input data."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2021-06-08 00:45:24 Training - Downloading the training image."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:45:51 INFO 139827178395456] Reading default configuration from /opt/amazon/lib/python3.7/site-packages/algorithm/default-input.json: {'base_network': 'vgg-16', 'use_pretrained_model': '0', 'num_classes': '', 'mini_batch_size': '32', 'epochs': '30', 'learning_rate': '0.001', 'lr_scheduler_step': '', 'lr_scheduler_factor': '0.1', 'optimizer': 'sgd', 'momentum': '0.9', 'weight_decay': '0.0005', 'overlap_threshold': '0.5', 'nms_threshold': '0.45', 'num_training_samples': '', 'image_shape': '300', '_tuning_objective_metric': '', '_kvstore': 'device', 'kv_store': 'device', '_num_kv_servers': 'auto', 'label_width': '350', 'freeze_layer_pattern': '', 'nms_topk': '400', 'early_stopping': 'False', 'early_stopping_min_epochs': '10', 'early_stopping_patience': '5', 'early_stopping_tolerance': '0.0', '_begin_epoch': '0'}\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:45:51 INFO 139827178395456] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'num_classes': '20', 'optimizer': 'rmsprop', 'base_network': 'resnet-50', 'num_training_samples': '16551', 'epochs': '1', 'image_shape': '300', 'learning_rate': '0.001', 'label_width': '350', 'mini_batch_size': '16', 'momentum': '0.9'}\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:45:51 INFO 139827178395456] Final configuration: {'base_network': 'resnet-50', 'use_pretrained_model': '0', 'num_classes': '20', 'mini_batch_size': '16', 'epochs': '1', 'learning_rate': '0.001', 'lr_scheduler_step': '', 'lr_scheduler_factor': '0.1', 'optimizer': 'rmsprop', 'momentum': '0.9', 'weight_decay': '0.0005', 'overlap_threshold': '0.5', 'nms_threshold': '0.45', 'num_training_samples': '16551', 'image_shape': '300', '_tuning_objective_metric': '', '_kvstore': 'device', 'kv_store': 'device', '_num_kv_servers': 'auto', 'label_width': '350', 'freeze_layer_pattern': '', 'nms_topk': '400', 'early_stopping': 'False', 'early_stopping_min_epochs': '10', 'early_stopping_patience': '5', 'early_stopping_tolerance': '0.0', '_begin_epoch': '0'}\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:45:51 INFO 139827178395456] Using default worker.\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:45:51 INFO 139827178395456] Loaded iterator creator application/x-image for content type ('application/x-image', '1.0')\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:45:51 INFO 139827178395456] Loaded iterator creator application/x-recordio for content type ('application/x-recordio', '1.0')\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:45:51 INFO 139827178395456] Loaded iterator creator image/jpeg for content type ('image/jpeg', '1.0')\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:45:51 INFO 139827178395456] Loaded iterator creator image/png for content type ('image/png', '1.0')\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:45:51 INFO 139827178395456] Checkpoint loading and saving are disabled.\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:45:51 INFO 139827178395456] nvidia-smi: took 0.083 seconds to run.\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:45:51 INFO 139827178395456] nvidia-smi identified 1 GPUs.\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:45:51 INFO 139827178395456] Number of GPUs being used: 1\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:45:51 WARNING 139827178395456] Training images are resized to image shape (3, 300, 300)\u001b[0m\n",
      "\u001b[34m[00:45:51] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.4.x_ecl_Cuda_9.x.135.0/AL2_x86_64/generic-flavor/src/src/io/iter_image_det_recordio.cc:283: ImageDetRecordIOParser: /opt/ml/input/data/train/train.rec, use 7 threads for decoding..\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2021-06-08 00:46:04 Training - Training image download completed. Training in progress.\u001b[34m[00:45:58] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.4.x_ecl_Cuda_9.x.135.0/AL2_x86_64/generic-flavor/src/src/io/iter_image_det_recordio.cc:340: ImageDetRecordIOParser: /opt/ml/input/data/train/train.rec, label padding width: 350\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:45:59 WARNING 139827178395456] Validation images are resized to image shape (3, 300, 300)\u001b[0m\n",
      "\u001b[34m[00:45:59] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.4.x_ecl_Cuda_9.x.135.0/AL2_x86_64/generic-flavor/src/src/io/iter_image_det_recordio.cc:283: ImageDetRecordIOParser: /opt/ml/input/data/validation/val.rec, use 7 threads for decoding..\u001b[0m\n",
      "\u001b[34m[00:46:01] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.4.x_ecl_Cuda_9.x.135.0/AL2_x86_64/generic-flavor/src/src/io/iter_image_det_recordio.cc:340: ImageDetRecordIOParser: /opt/ml/input/data/validation/val.rec, label padding width: 350\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:46:03 INFO 139827178395456] Number of GPUs being used: 1\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:46:03 INFO 139827178395456] Using [gpu(0)] as training context.\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:46:03 INFO 139827178395456] Number of GPUs being used: 1\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:46:03 INFO 139827178395456] Create Store: device\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:46:03 INFO 139827178395456] Using (gpu(0)) as training context.\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:46:03 INFO 139827178395456] Start training from the model in the model channel.\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:46:03 INFO 139827178395456] Found a model archive file. Extracting model.tar.gz\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:46:07 INFO 139827178395456] Using model weights from file /tmp/tmp2usviwg7/model_algo_1.\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:46:07 INFO 139827178395456] done loading checkpoint\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/08/2021 00:46:17 INFO 139827178395456] Creating a new state instance.\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623113177.8059494, \"EndTime\": 1623113177.8060446, \"Dimensions\": {\"Algorithm\": \"AWS/Object Detection\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"Meta\": \"init_train_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Total Batches Seen\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Max Records Seen Between Resets\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Max Batches Seen Between Resets\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Reset Count\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Number of Records Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Number of Batches Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[00:46:17] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.4.x_ecl_Cuda_9.x.135.0/AL2_x86_64/generic-flavor/src/src/operator/nn/./cudnn/./cudnn_algoreg-inl.h:97: Running performance tests to find the best convolution algorithm, this can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/08/2021 00:46:40 INFO 139827178395456] Epoch:    0, batches:    100, num_examples:   1600, 70.8 samples/sec, epoch time so far:  0:00:22.609019\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/08/2021 00:46:57 INFO 139827178395456] Epoch:    0, batches:    200, num_examples:   3200, 80.0 samples/sec, epoch time so far:  0:00:40.021103\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/08/2021 00:47:15 INFO 139827178395456] Epoch:    0, batches:    300, num_examples:   4800, 83.0 samples/sec, epoch time so far:  0:00:57.816701\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/08/2021 00:47:33 INFO 139827178395456] Epoch:    0, batches:    400, num_examples:   6400, 85.1 samples/sec, epoch time so far:  0:01:15.199314\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/08/2021 00:47:50 INFO 139827178395456] Epoch:    0, batches:    500, num_examples:   8000, 86.3 samples/sec, epoch time so far:  0:01:32.655668\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/08/2021 00:48:07 INFO 139827178395456] Epoch:    0, batches:    600, num_examples:   9600, 87.1 samples/sec, epoch time so far:  0:01:50.168201\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/08/2021 00:48:25 INFO 139827178395456] Epoch:    0, batches:    700, num_examples:   11200, 87.5 samples/sec, epoch time so far:  0:02:08.007687\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/08/2021 00:48:43 INFO 139827178395456] Epoch:    0, batches:    800, num_examples:   12800, 87.8 samples/sec, epoch time so far:  0:02:25.761494\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/08/2021 00:49:00 INFO 139827178395456] Epoch:    0, batches:    900, num_examples:   14400, 88.4 samples/sec, epoch time so far:  0:02:42.912719\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/08/2021 00:49:18 INFO 139827178395456] Epoch:    0, batches:    1000, num_examples:   16000, 88.6 samples/sec, epoch time so far:  0:03:00.500626\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:49:23 INFO 139827178395456] #quality_metric: host=algo-1, epoch=0, batch=1035 train cross_entropy <loss>=(0.8369018467318383)\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:49:23 INFO 139827178395456] #quality_metric: host=algo-1, epoch=0, batch=1035 train smooth_l1 <loss>=(0.3878045983785223)\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:49:23 INFO 139827178395456] Round of batches complete\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:49:23 INFO 139827178395456] Updated the metrics\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/08/2021 00:50:19 INFO 139827178395456] #quality_metric: host=algo-1, epoch=0, validation mAP <score>=(0.38044882677318465)\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:50:19 INFO 139827178395456] Updating the best model with validation-mAP=0.38044882677318465\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:50:19 INFO 139827178395456] Saved checkpoint to \"/opt/ml/model/model_algo_1-0000.params\"\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:50:19 INFO 139827178395456] #progress_metric: host=algo-1, completed 100.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623113177.8064299, \"EndTime\": 1623113419.280929, \"Dimensions\": {\"Algorithm\": \"AWS/Object Detection\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 0, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Total Batches Seen\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Max Records Seen Between Resets\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Max Batches Seen Between Resets\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Reset Count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Number of Records Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Number of Batches Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:50:19 WARNING 139827178395456] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:50:19 INFO 139827178395456] Saved checkpoint to \"/opt/ml/model/model_algo_1-0000.params\"\u001b[0m\n",
      "\u001b[34m[06/08/2021 00:50:19 INFO 139827178395456] Test data is not provided.\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623113151.2431524, \"EndTime\": 1623113420.0309906, \"Dimensions\": {\"Algorithm\": \"AWS/Object Detection\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"epochs\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"setuptime\": {\"sum\": 8.00943374633789, \"count\": 1, \"min\": 8.00943374633789, \"max\": 8.00943374633789}, \"totaltime\": {\"sum\": 268897.3400592804, \"count\": 1, \"min\": 268897.3400592804, \"max\": 268897.3400592804}}}\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2021-06-08 00:51:11 Uploading - Uploading generated training model\n",
      "2021-06-08 00:51:11 Completed - Training job completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training seconds: 397\n",
      "Billable seconds: 397\n"
     ]
    }
   ],
   "source": [
    "new_od_model.fit(inputs=new_data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4f7f30",
   "metadata": {
    "papermill": {
     "duration": 0.069615,
     "end_time": "2021-06-08T00:51:24.672726",
     "exception": false,
     "start_time": "2021-06-08T00:51:24.603111",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Instead of repeating the first `5` epochs from the previous job, we started the training with the trained model and improved the results with only one epoch. In this way, models pre-trained in SageMaker can now be re-used to improve the training efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555a938b",
   "metadata": {
    "papermill": {
     "duration": 0.06953,
     "end_time": "2021-06-08T00:51:24.811993",
     "exception": false,
     "start_time": "2021-06-08T00:51:24.742463",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Hosting\n",
    "Once the training is done, we can deploy the trained model as an Amazon SageMaker real-time hosted endpoint. This will allow us to make predictions (or inference) from the model. Note that we don't have to host on the same instance (or type of instance) that we used to train. Training is a prolonged and compute heavy job that require a different of compute and memory requirements that hosting typically do not. We can choose any type of instance we want to host the model. In our case we chose the `ml.p3.2xlarge` instance to train, but we choose to host the model on the less expensive cpu instance, `ml.m4.xlarge`. The endpoint deployment can be accomplished as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1fb5c22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T00:51:24.957030Z",
     "iopub.status.busy": "2021-06-08T00:51:24.956481Z",
     "iopub.status.idle": "2021-06-08T00:58:57.006374Z",
     "shell.execute_reply": "2021-06-08T00:58:57.006854Z"
    },
    "papermill": {
     "duration": 452.125515,
     "end_time": "2021-06-08T00:58:57.007015",
     "exception": false,
     "start_time": "2021-06-08T00:51:24.881500",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!"
     ]
    }
   ],
   "source": [
    "object_detector = new_od_model.deploy(initial_instance_count=1, instance_type=\"ml.m4.xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5139047f",
   "metadata": {
    "papermill": {
     "duration": 0.074415,
     "end_time": "2021-06-08T00:58:57.155487",
     "exception": false,
     "start_time": "2021-06-08T00:58:57.081072",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Inference\n",
    "Now that the trained model is deployed at an endpoint that is up-and-running, we can use this endpoint for inference. To do this, let us download an image from [PEXELS](https://www.pexels.com/) which the algorithm has so-far not seen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4f6db03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T00:58:57.309477Z",
     "iopub.status.busy": "2021-06-08T00:58:57.308934Z",
     "iopub.status.idle": "2021-06-08T00:58:57.537323Z",
     "shell.execute_reply": "2021-06-08T00:58:57.536722Z"
    },
    "papermill": {
     "duration": 0.307737,
     "end_time": "2021-06-08T00:58:57.537449",
     "exception": false,
     "start_time": "2021-06-08T00:58:57.229712",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-06-08 00:58:57--  https://images.pexels.com/photos/980382/pexels-photo-980382.jpeg\r\n",
      "Resolving images.pexels.com (images.pexels.com)... 104.17.209.102, 104.17.208.102, 2606:4700::6811:d166, ...\r\n",
      "Connecting to images.pexels.com (images.pexels.com)|104.17.209.102|:443... connected.\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP request sent, awaiting response... 503 Service Temporarily Unavailable\r\n",
      "2021-06-08 00:58:57 ERROR 503: Service Temporarily Unavailable.\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!wget -O test.jpg https://images.pexels.com/photos/980382/pexels-photo-980382.jpeg\n",
    "file_name = \"test.jpg\"\n",
    "\n",
    "with open(file_name, \"rb\") as image:\n",
    "    f = image.read()\n",
    "    b = bytearray(f)\n",
    "    ne = open(\"n.txt\", \"wb\")\n",
    "    ne.write(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cec48d7",
   "metadata": {
    "papermill": {
     "duration": 0.075077,
     "end_time": "2021-06-08T00:58:57.687451",
     "exception": false,
     "start_time": "2021-06-08T00:58:57.612374",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let us use our endpoint to try to detect objects within this image. Since the image is `jpeg`, we use the appropriate `content_type` to run the prediction job. The endpoint returns a JSON file that we can simply load and peek into."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d125d9ea",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9fb4f950",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T00:58:57.844866Z",
     "iopub.status.busy": "2021-06-08T00:58:57.843947Z",
     "iopub.status.idle": "2021-06-08T00:58:57.856391Z",
     "shell.execute_reply": "2021-06-08T00:58:57.855386Z"
    },
    "papermill": {
     "duration": 0.094303,
     "end_time": "2021-06-08T00:58:57.856637",
     "exception": true,
     "start_time": "2021-06-08T00:58:57.762334",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "can't set attribute",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-c373d0a66cb7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mobject_detector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"image/jpeg\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobject_detector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdetections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: can't set attribute"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "object_detector.content_type = \"image/jpeg\"\n",
    "results = object_detector.predict(b)\n",
    "detections = json.loads(results)\n",
    "print(detections)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91257de",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "The results are in a format that is similar to the .lst format with an addition of a confidence score for each detected object. The format of the output can be represented as `[class_index, confidence_score, xmin, ymin, xmax, ymax]`. Typically, we don't consider low-confidence predictions.\n",
    "\n",
    "We have provided additional script to easily visualize the detection outputs. You can visualize the high-confidence predictions with bounding box by filtering out low-confidence detections using the script below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7ce8b1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def visualize_detection(img_file, dets, classes=[], thresh=0.6):\n",
    "    \"\"\"\n",
    "    visualize detections in one image\n",
    "    Parameters:\n",
    "    ----------\n",
    "    img : numpy.array\n",
    "        image, in bgr format\n",
    "    dets : numpy.array\n",
    "        ssd detections, numpy.array([[id, score, x1, y1, x2, y2]...])\n",
    "        each row is one object\n",
    "    classes : tuple or list of str\n",
    "        class names\n",
    "    thresh : float\n",
    "        score threshold\n",
    "    \"\"\"\n",
    "    import random\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.image as mpimg\n",
    "\n",
    "    img = mpimg.imread(img_file)\n",
    "    plt.imshow(img)\n",
    "    height = img.shape[0]\n",
    "    width = img.shape[1]\n",
    "    colors = dict()\n",
    "    for det in dets:\n",
    "        (klass, score, x0, y0, x1, y1) = det\n",
    "        if score < thresh:\n",
    "            continue\n",
    "        cls_id = int(klass)\n",
    "        if cls_id not in colors:\n",
    "            colors[cls_id] = (random.random(), random.random(), random.random())\n",
    "        xmin = int(x0 * width)\n",
    "        ymin = int(y0 * height)\n",
    "        xmax = int(x1 * width)\n",
    "        ymax = int(y1 * height)\n",
    "        rect = plt.Rectangle(\n",
    "            (xmin, ymin),\n",
    "            xmax - xmin,\n",
    "            ymax - ymin,\n",
    "            fill=False,\n",
    "            edgecolor=colors[cls_id],\n",
    "            linewidth=3.5,\n",
    "        )\n",
    "        plt.gca().add_patch(rect)\n",
    "        class_name = str(cls_id)\n",
    "        if classes and len(classes) > cls_id:\n",
    "            class_name = classes[cls_id]\n",
    "        plt.gca().text(\n",
    "            xmin,\n",
    "            ymin - 2,\n",
    "            \"{:s} {:.3f}\".format(class_name, score),\n",
    "            bbox=dict(facecolor=colors[cls_id], alpha=0.5),\n",
    "            fontsize=12,\n",
    "            color=\"white\",\n",
    "        )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4095bc",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "For the sake of this notebook, we trained the model with only one epoch. This implies that the results might not be optimal. To achieve better detection results, you can try to tune the hyperparameters and train the model for more epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52e0c1a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "object_categories = [\n",
    "    \"aeroplane\",\n",
    "    \"bicycle\",\n",
    "    \"bird\",\n",
    "    \"boat\",\n",
    "    \"bottle\",\n",
    "    \"bus\",\n",
    "    \"car\",\n",
    "    \"cat\",\n",
    "    \"chair\",\n",
    "    \"cow\",\n",
    "    \"diningtable\",\n",
    "    \"dog\",\n",
    "    \"horse\",\n",
    "    \"motorbike\",\n",
    "    \"person\",\n",
    "    \"pottedplant\",\n",
    "    \"sheep\",\n",
    "    \"sofa\",\n",
    "    \"train\",\n",
    "    \"tvmonitor\",\n",
    "]\n",
    "\n",
    "# Setting a threshold 0.20 will only plot detection results that have a confidence score greater than 0.20.\n",
    "threshold = 0.20\n",
    "\n",
    "# Visualize the detections.\n",
    "visualize_detection(file_name, detections[\"prediction\"], object_categories, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4227963c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Delete the Endpoint\n",
    "Having an endpoint running will incur some costs. Therefore as a clean-up job, we should delete the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3288f751",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(object_detector.endpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.",
  "papermill": {
   "default_parameters": {},
   "duration": 2614.489069,
   "end_time": "2021-06-08T00:58:58.339206",
   "environment_variables": {},
   "exception": true,
   "input_path": "object_detection_incremental_training.ipynb",
   "output_path": "/opt/ml/processing/output/object_detection_incremental_training-2021-06-08-00-11-27.ipynb",
   "parameters": {
    "kms_key": "arn:aws:kms:us-west-2:521695447989:key/6e9984db-50cf-4c7e-926c-877ec47a8b25"
   },
   "start_time": "2021-06-08T00:15:23.850137",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}